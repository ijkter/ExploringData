# Machine Learning

## 特征工程

### 特征归一化（Feature Normalization）

#### 为什么要对数值类型的特征归一化？

为了消除特征之间的量纲影响，通过对特征进行归一化处理，可以将所有的特征都统一到一个大致相同的数值区间内，使得不同指标之间具有可比性。

#### 特征归一化的方法

- 线性函数归一化（Min-Max Scaling）
  
  对原始数据进行线性变化，使其结果映射到[0, 1]的范围，实现对原数据的等比缩放。假设最大值为 $X_{max}$，最小值为 $X_{min}$，那么归一化公式为：

  $$ X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$$

- 零均值归一化（Z-Score Normalization）
  
  将原始数据映射到均值为 0，标准差为 1 的分布上。假设均值为 $\mu$，标准差为 $\sigma$，那么归一化公式为：

  $$ z = \frac{x - \mu}{\sigma}$$

#### 为什么通过梯度下降法求解的模型通常需要归一化？

在学习速览相同的情况下，通过将量纲差距较大的特征归一化，可以使特征的更细速度变得更为一致，容易更快地通过梯度下降找到最优解。

### 类别性特征

#### 数据处理时如何处理类别型特征

- 序号编码（Ordinal Encoding）
  
  序号编码常用于处理类别间**具有大小关系**的特征，按照大小关系对类别型特征赋予一个数值 ID，使数据转换后依然保留大小关系。

- 独热编码（One-Hot Encoding）

  独热编码通常用于处理类别间**不具有大小关系**的特征，将特征转变为一个稀疏向量，只有某一维取值为 1，其他位置取值为 0。对于类别取值较多的情况要注意以下问题：
  - 使用稀疏向量来节省空间
  - 配合特征选择降低维度

- 二进制编码（Binary Encoding）

  首先用序号编码给每个类别赋予一个类别 ID，然后用类别 ID 对应的二进制编码作为结果。利用二进制对 ID 进行映哈希映射，最终得到的 0/1 特征向量，相较独热码节省了存储空间。

### 高维特征

#### 高维特征可能出现的问题

- 在 K 近邻算法中，高维空间下两点之间的距离很难得到有效衡量
- 在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合
- 通常只有部分维度是对分类、预测有帮助

#### 组合特征

为提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

#### 如何有效地找到组合特征

- 基于决策树的特征组合寻找方法
  
  采用梯度提升决策树的方法，每次都在之前构建的决策树的残差上构建下一棵决策树。根据构造出来的决策树，每一条从根节点到叶节点的路径都可以看成一种特征组合。

### 文本表示模型

#### 词袋模型（Bag of Words）

- 核心思想

  将每篇文章看成一袋子词，并忽略每个词出现的顺序。

- 过程
  
  将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重（常由 TF-IDF 计算而得）则反映了这个词在原文章的重要程度。

- 缺点
  
  将文章进行单次级别划分，每个单词所表达的含义与其组合含义可能相去甚远。

#### TF-IDF（Term Frequency-Inverse Document Frequency）

- 核心思想
  
  如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分某篇文章特殊语义的贡献较小，因此对其权重做一定惩罚。

- 公式：
  $$ TF - IDF(t,d)=TF(t,d) \times IDF(t) $$
  其中，TF(t,d) 为单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，表示为：
  $$ IDF(t)=\log \frac{文章总数}{包含单词 t 的文章总数 + 1}$$


#### N-gram 模型

- 核心思想

  在词袋模型的基础上，将连续出现的 n 个词（n ≤ N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中。

- 词干抽取（Word Stemming）
  
  将不同词性的单词统一成为同一词干的形式。

#### 主题模型（Topic Model）

用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。

#### 词嵌入模型（Word Embedding）

- 核心思想
  
  将每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）。K 维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那么直观。

### Word2Vec

常用词嵌入模型之一，是一种浅层的神经网络模型，有两种网络结构：
- CBOW（Continues Bag of Words）：根据上下文出现的词语来预测当前词的生成概率
- Skip-gram：根据当前词来预测上下文中各词的生成概率

### 训练数据不足的问题

#### 处理方法

根据模型信息的来源：
1. 训练数据中蕴含的信息
2. 模型的形成过程中（包括构造、学习、推理等）提供的先验信息

故在训练数据不足时，模型需要更多的先验信息：
- 先验信息作用在模型上，例如让模型采用特定的内在结构、条件假设或添加其他一些约束条件
- 先验信息直接施加在数据集上，即根据特定的先验假设去调整、变换或拓展训练数据，让其展现出更多的、更有的信息，以利于后续模型的训练和学习

#### 图像训练数据不足时的处理方法

- 基于模型的方法
  
  简化模型（将非线性模型简化为线性模型）、添加约束项以缩小假设空间（如 L1/L2 正则项）、集成学习、Dropout 超参数等

- 基于数据的方法
  
  - 一定程度的随机旋转、平移、缩放、裁剪、填充、左右翻转等
  - 对图像中的像素添加噪声扰动，比如：椒盐噪声、高斯白噪声等
  - 改变图像的颜色、亮度、清晰度、对比度、锐度等

## 模型评估

### 评估指标

#### 准确率（Accuracy）

- 定义
  
  分类正确的样本占总样本个数的比例，即

  $$ Accuracy = \frac{n_{correct}}{n_{}} $$

  其中 $n_{correct}$ 为被正确分类的样本个数，$n_{total}$ 为总样本的个数。

- 局限性
  
  当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的主要因素。如：负样本占整体样本的 99%，将全体样本都预测为负样本都可以获得 99% 的准确率。

#### 精确率（Precision）

- 定义

  分类正确的正样本个数占分类器判定为正样本的样本个数的比例。

#### 召回率（Recall）


#### F1 score

- 精确率和召回率的调和平均值，定义为：F1 = $\frac{2 \times precision \times recall}{precision + recall}$
- 综合地反映一个排序模型的性能

#### 均方根误差（Root Mean Square Error， RMSE）

- 计算公式

  $$ RMSE = \sqrt{\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{n}} $$
  
  其中，$y_i$ 是第 i 个样本点的真实值，$\hat{y_i}$ 是第 i 个样本点的预测值，n 是样本点的个数。

- 局限性
  
  若存在个别偏离程度非常大的利离群点时，即使离群点数量非常少，也会让 RMSE 指标变得很差。